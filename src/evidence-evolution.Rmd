---
title: "Evidence Synthesis"
author: "Anonymous"
date: '2024-09-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(patchwork) # alignment of figures

library(ggdag) # visualization and analysis of DAGs

library(lme4) # fitting linear mixed models
library(brms) # fitting Bayesian hierarchical models
library(marginaleffects) # visualizing marginal effects of Bayesian models
```

This notebook demonstrates the application of the evidence evolution framework by applying it to existing evidence from the domain of requirements quality research.
In this application, we put the evidence from three primary studies investigating the same phenomenon into relation with each other.

## Evidence e1: Original Study

The original study by Femmer et al.[^1] studied the impact that the use of passive voice has on the domain modeling activity.
They formulate the following research question:

> Is the use of passive sentences in requirements harmfulfor domain modelling?

### Hypothesis h1

The authors imply a simple hypothesis $h_1:=pv \rightarrow Asc^-$ (i.e., the use of passive voice $pv$ impacts the number of missing associations $A^-$).
The original stud[^1] also investigates the impact on the number of missing actors and domain objects, but these are out of scope for this synthesis.
The causal assumptions can be visualized in the following directed, acyclic graph (DAG).

```{r hypothesis-h1-dag}
h1.dag <- dagify(
  mact ~ pv,
  mobj ~ pv,
  masc ~ pv,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", masc="missing.associations"),
  coords = list(x=c(pv=1, mact=2, mobj=2, masc=2),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1))
)

ggdag_status(h1.dag, 
             use_labels = "label", 
             text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

### Data d1

To investigate this hypothesis, the authors perform a parallel-design controlled experiment with 15 university students as participants.
The participants are randomly divided into two groups and receive seven single-sentence requirements specifications, either written in active or passive voice.
Their task was to generate a domain model from each of the requirements specifications.
The authors then counted the number of missing actors, domain objects, and associations (collectively called: _elements_ of the domain model) in the resulting domain model.

```{r data-d1-loading}
d1 <- read.csv(file="../data/femmer-2014.csv")

# cast categorical variables to actual factors
cat.exp <- c("no experience", "up to 6 months", "6 to 12 months", "more than 12 months")
d1 <- d1 %>% 
  mutate(
    ExpProgAca <- factor(ExpProgAca, levels=cat.exp, ordered=TRUE),
    ExpProgInd <- factor(ExpProgInd, levels=cat.exp, ordered=TRUE), 
    ExpSEAca <- factor(ExpSEAca, levels=cat.exp, ordered=TRUE),
    ExpSEInd <- factor(ExpSEInd, levels=cat.exp, ordered=TRUE),
    ExpREAca <- factor(ExpREAca, levels=cat.exp, ordered=TRUE),
    ExpREInd <- factor(ExpREInd, levels=cat.exp, ordered=TRUE)
  )
```

The following figure shows a simple visualization of the distribution of the number of missing domain elements (actors, domain objects, and associations) for the `active` and `passive` treatment of the requirements sentences.

```{r data-d1-visualization}
d1 %>% 
  select(passive, actors.missing, objects.missing, associations.missing) %>% 
  pivot_longer(
    cols = c(actors.missing, objects.missing, associations.missing),
    names_to = "dependent.variable",
    values_to = "count"
  ) %>% 
  ggplot(aes(y=dependent.variable, x=count, color=passive)) +
    geom_boxplot()
```

### Method m1

Finally, the authors performed a null-hypothesis significance test (NHST) to see if there is a statistically significant difference in the number of missing elements between the active or passive group.
Of particular interest to this synthesis is the difference in the number of missing _associations_.
The authors use the Mann-Whitney U test (i.e., a Wilcoxon rank-sum test) with a 95% confidence interval since the data is not normally distributed.

```{r method-m1-original}
wilcox.test(
  x = filter(d1, passive==1)[["associations.missing"]],
  y = filter(d1, passive==0)[["associations.missing"]],
  conf.int = TRUE,
  conf.level = 0.95,
  paired = FALSE
)
```

Evidence $e_1=E(h_1, d_1, m_1)$ concludes that passive voice has a statistically significant impact on the number of missing associations $Asc^-$ with a p-value of around 0.001.

## Evidence e2: Reanalysis 

Since statistical null-hypothesis significance tests are equivalent to a linear model,[^4] we can replace the Wilcoxon rank sum test with a linear model, i.e., `stats::lm`.

### Method m2

We have two means (one per `passive` value) and a non-parametric outcome variable.
We do not need to transform the variable `associations.missing` to ranks since the values are already equivalent to ranks.
This way, we do not lose any information through the rank transformation and the resulting confidence interval remains interpretable.

```{r method-m2}
# explicit definition of the causal hypothesis
h1 <- associations.missing ~ passive

# defining a linear model with the same causal hypothesis h1 and the previous data d1
e2 <- lm(
  formula = h1, 
  data = d1)

# investigating the model parameters
summary(e2)
```

From the parameters of the fitted linear model, we can obtain the confidence interval of the factor `passive` on the response variable `Asc^-`.

```{r evidence-e2}
confint(
  e2, 
  parm="passiveTRUE", 
  level=0.95)
```

The 95%-confidence interval is strictly positive, i.e., passive voice has a significant impact on the number of missing associations.
As such, $e_1=E(h_1, d_1, m_2)$ agrees with $e_1$.
This reanalysis, also referred to as a _test of robustness_, suggests that the conclusions drawn from $e_1$ about the impact of passive voice on the number of missing associations from a domain model is valid.

## Evidence e3: Revision

A follow-up study[^2] voiced concerns about the causal assumptions in the original study.[^1]

### Hypothesis h2

The study revised several causal assumptions about the original study.
One of them is that missing actors and objects in a domain model might increase to missing associations, since at least one of the nodes in the graph is missing.

```{r hypothesis-h2-dag}
h2.dag <- dagify(
  mact ~ pv,
  mobj ~ pv,
  masc ~ pv + mact + mobj,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", masc="missing.associations"),
  coords = list(x=c(pv=0.7, mact=2, mobj=2, masc=2),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1))
)

ggdag_status(h2.dag, use_labels = "label", text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

### Conclusion

Deriving a linear model from this DAG results in the following formula and confidence interval.

```{r evidence-e3}
# explicit definition of the new causal hypothesis h2
h2 <- associations.missing ~ passive + actors.missing + objects.missing

# defining a linear model with the new causal hypothesis h2 and the previous data d1
e3 <- lm(formula = h2, data = d1)
confint(e3, parm="passiveTRUE", level=0.95)
```

Despite revised causal assumption, the conclusion is still similar to $e2$, though a bit more cautious.
To determine which model explains the observed data better, though we can investigate the coefficient of determination $R^2$.
This coefficient represents the proportion of variation in the dependent variable that can be attributed to the independent variable

```{r comparison-e2-e3}
summary(e3)
```

The coefficient of determination (Multiple R-squared) of $e_3$ is greater than of $e_2$.

$$R^2_{e_2} = 0.1041 < 0.1606 = R^2_{e_3}$$

Because $h_2$ explains the observed data $d_1$ better than $h_1$, we assume it to be the hypothesis with the highest internal validity at this point.
Additionally, the coefficients indicate that the number of missing objects may have a statistically significant impact on the number of missing associations, confirming the additional causal assumption.

## Evidence e4: Revision and Reanalysis

Another revision in scope of the follow-up study[^2] was the consideration of random effects.

### Hypothesis h3

The assumptions are based on the fact that the relatively small sample of 15 participants included both better and worse performing students, and that the individual complexity of each requirement may not be comparable.

```{r hypothesis-h3-dag}
h3.dag <- dagify(
  mact ~ pv + skill + req,
  mobj ~ pv + skill + req,
  masc ~ pv + mact + mobj + skill + req,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", 
             masc="missing.associations", skill="Individual Skill", 
             req="Requirements Complexity"),
  coords = list(x=c(pv=0.7, mact=2, mobj=2, masc=2, skill=0, req=0),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1, skill=1.5, req=0.5))
)

ggdag_status(h3.dag, 
             use_labels = "label", 
             text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

These two factors are typically represented as random factors in a linear model.

```{r hypothesis-h3-formula}
# explicit definition of the new causal hypothesis h2
h3 <- associations.missing ~ passive + # main factor of interest
  actors.missing + objects.missing + # confounders
  (1|PID) + (1|RID) # random effects
```

### Conclusion

Because the inclusion of random factors in a linear model requires using a linear mixed model (LMM), the revision entails a reanalysis, i.e., replacing $h_2$ with $h_3$ also necessitates replacing $m_2$ (LM) with $m_3$ (LMM) via `lme3:lmer`.

```{r evidence-e4}
e4 <- lmer(formula = h3, data = d1)

confint(e4, level=0.95)
```

The confidence interval of the `passive` factor from $e_4$ is a lot more cautious than in $e_3$ but still strictly positive.
Evidence $e_4$ still supports $e_3$ in that passive voice has a statistically significant impact on the number of missing associations in a domain model.
Additionally, the factor `objects.missing` remains strictly positive, supporting the assumption that the number of missing objects impacts the number of missing associations.

```{r evidence-e4-mobj}
confint(e4, parm="objects.missing", level=0.95)
```

## Evidence e5: Revision

The revision[^2] of the hypothesis $h_1$ of the original study included one more assumption.
Academic and industrial experience in requirements engineering might reduce the number of missing associations, as they increase the likelihood of previous modeling experience.

### Hypothesis h4

These additional assumptions can be visualized in the following DAG.

```{r hypothesis-h4-dag}
h4.dag <- dagify(
  mact ~ pv + exp.aca + exp.ind + skill + req,
  mobj ~ pv + exp.aca + exp.ind + skill + req,
  masc ~ pv + mact + mobj + exp.aca + exp.ind + skill + req,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", 
             masc="missing.associations", exp.aca="Academic Experience in RE", 
             exp.ind="Industrial Experience in RE", skill="Individual Skill", 
             req="Requirements Complexity"),
  coords = list(x=c(pv=0.7, mact=2, mobj=2, masc=2, exp.aca=0.3, exp.ind=0.3, skill=0, req=0),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1, exp.aca=-0.5, exp.ind=0, skill=1.5, req=0.5))
)

ggdag_status(h4.dag, use_labels = "label",text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

These two factors are added as predictors to the formula for the linear model.

```{r hypothesis-h4-formula}
h4 <- associations.missing ~ passive + 
  actors.missing + objects.missing + 
  ExpREAca + ExpREInd + 
  (1|PID) + (1|RID)
```

### Conclusion

#### Model comparison

Fitting a linear mixed model to this new hypothesis produces evidence $e_5$.
To assess, whether the new hypothesis $h_4$ has a greater internal validity than $h_3$, we compare the $R^2$ values of the two fitted models. 

```{r evidence-e5}
e5 <- lmer(formula = h4, data = d1)

performance::r2(e4)
performance::r2(e5)
```

Since $R^2_{e_5} > R^2_{e_4}$, we conclude that $h_4$ is a better causal explanation of data $d_1$ than $h_3$.

#### Factor evaluation

When evaluating the conclusion of $e_5$ regarding the impact of the use of passive voice, we again calculate the 95% confidence interval.

```{r evidence-e5-passive}
confint(e5, parm="passiveTRUE", level=0.95)
```

This confidence interval is significantly different to the previous one. 
Though mostly positive, the confidence interval intersects 0 and is therefore not significant anymore.
Additionally, the only remaining factor with a strictly positive confidence interval is the number of missing objects `objects.missing`.

```{r evidence-e5-mobj}
confint(e5, parm="objects.missing", level=0.95)
```

The new evidence $e_5=E(h_4, d_1, m_3)$ is more internally valid than previous evidences and rejects the hypothesis that passive voice has a significant impact on the number of missing associations in domain modeling.

## Evidence e6: Reanalysis

Finally, the follow-up study also suggested to use a Bayesian modeling approach instead of a frequentist.
Bayesian approaches are understood to be more robust, sophisticated, and preserve uncertainty.[^5]

### Method m4

The Bayesian model, other than the linear mixed model, can represent the hypothesis properly as a Binomial probability distribution.

```{r hypothesis-h3-bayesian}
h4.b <- (associations.missing | trials(associations.expected) ~ 
           passive + actors.missing + objects.missing +
           ExpREAca + ExpREInd + (1|RID) + (1|PID))
```

For each of the selected predictors, we select an uninformative prior distribution that encodes our previous knowledge about the causal phenomenon.

```{r method-m4-priors}
priors <- c(prior(normal(-1, 1), class = Intercept),
            prior(normal(0, 1), class = b),
            prior(weibull(2, 1), class = sd, coef = Intercept, group = RID),
            prior(weibull(2, 1), class = sd, coef = Intercept, group = PID),
            prior(exponential(1), class = sd))
```

To assert that the priors are feasible, we sample from the priors without training the Bayesian model with the data.

```{r method-m4-model-prior, message=FALSE, warning=FALSE}
e6.prior <-
  brm(data = d1, family = binomial, h4.b, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/e6.prior"
  )
```

We visualize the results to graphically check the priors eligibility.

```{r method-m4-priorpc}
ndraws <- 400
brms::pp_check(e6.prior, type="bars", ndraws=ndraws)
```

The distribution of the predicted values for the response variable ($y_{rep}$) overlaps with the actually observed response variable distribution ($y$), confirming the feasibility of the priors.
Given the feasibility of the priors, we train the regression model. 
This process updates the prior distributions of all predictor coefficients.

```{r method-m4-model, message=FALSE, warning=FALSE}
e6 <-
  brm(data = d1, family = binomial, h4.b, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4,
    file = "fits/e6"
  )
```

We sample from the posterior distributions similar to the prior predictive check.

```{r method-m4-postpc}
brms::pp_check(e6, type="bars", ndraws=ndraws)
```

### Conclusion

We evaluate the trained model. 
An initial glance at the predictor coefficients shows their posterior distribution.

```{r evidence-e6-summary}
# numeric summary of the confidence intervals in the trained model
# summary(e6)

# visual summary of the confidence intervals in the trained model
mcmc_plot(e6)
```

The confidence interval of the factor passive voice `b_passiveTRUE` again overlaps 0 and is, therefore, not significant.
Again, the confidence interval of the number of missing objects `b_objects.missing` is strictly positive, indicating a significant impact.
Similarly, the intercepts of the random effects per participant `sd_PID__Intercept` and per requirements `sd_RID__Intercept` are strictly positive, also hinting at a significant factor.

Confidence intervals do not show the full picture, though.
To involve all uncertainty that the model has picked up during the training in the conclusion, we can visualize the marginal effect of passive voice via `brms::conditional_effects`.

```{r evidence-e6-marginaleffect}
conditional_effects(e6, effects="passive", trials=1)
```

The overlap between the two confidence intervals shows that the use of passive voice does not significantly cause more missing associations.

[^1]: Femmer, H., Kučera, J., & Vetrò, A. (2014, September). On the impact of passive voice requirements on domain modelling. In Proceedings of the 8th ACM/IEEE international symposium on empirical software engineering and measurement (pp. 1-4). https://doi.org/10.1145/2652524.2652554
[^2]: Frattini, J., Fucci, D., Torkar, R., & Mendez, D. (2024, April). A second look at the impact of passive voice requirements on domain modeling: Bayesian reanalysis of an experiment. In Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering (pp. 27-33). https://doi.org/10.1145/3643664.3648211
[^3]: Frattini, J., Fucci, D., Torkar, R., Montgomery, L., Unterkalmsteiner, M., Fischbach, J., & Mendez, D. (2024). Applying Bayesian Data Analysis for Causal Inference about Requirements Quality: A Replicated Experiment. arXiv preprint: https://arxiv.org/abs/2401.01154
[^4]: https://lindeloev.github.io/tests-as-linear/
[^5]: Furia, C. A., Feldt, R., & Torkar, R. (2019). Bayesian data analysis in empirical software engineering research. IEEE Transactions on Software Engineering, 47(9), 1786-1810.