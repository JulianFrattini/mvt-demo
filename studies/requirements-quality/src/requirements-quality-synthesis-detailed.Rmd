---
title: "Evidence Synthesis"
author: "Anonymous"
date: '2024-09-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(patchwork) # alignment of figures

library(ggdag) # visualization and analysis of DAGs
library(ggstats) # visualization of coefficient plots

library(lme4) # fitting linear mixed models
library(brms) # fitting Bayesian hierarchical models
library(marginaleffects) # visualizing marginal effects of Bayesian models
```

This notebook demonstrates the application of the evidence evolution framework by applying it to existing evidence from the domain of requirements quality research.
In this application, we put the evidence from three primary studies investigating the same phenomenon into relation with each other.

## Evidence e1: Original Study

The original study by Femmer et al.[^1] studied the impact that the use of passive voice has on the domain modeling activity.
They formulate the following research question:

> Is the use of passive sentences in requirements harmfulfor domain modelling?

### Hypothesis h1

The authors imply a simple hypothesis $h_1:=pv \rightarrow Asc^-$ (i.e., the use of passive voice $pv$ impacts the number of missing associations $A^-$).
The original stud[^1] also investigates the impact on the number of missing actors and domain objects, but these are out of scope for this synthesis.
The causal assumptions can be visualized in the following directed, acyclic graph (DAG).

```{r hypothesis-h1-dag}
h1.dag <- dagify(
  mact ~ pv,
  mobj ~ pv,
  masc ~ pv,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", masc="missing.associations"),
  coords = list(x=c(pv=1, mact=2, mobj=2, masc=2),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1))
)

ggdag_status(h1.dag, 
             use_labels = "label", 
             text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

### Data d1

To investigate this hypothesis, the authors perform a parallel-design controlled experiment with 15 university students as participants.
The participants are randomly divided into two groups and receive seven single-sentence requirements specifications, either written in active or passive voice.
Their task was to generate a domain model from each of the requirements specifications.
The authors then counted the number of missing actors, domain objects, and associations (collectively called: _elements_ of the domain model) in the resulting domain model.

```{r data-d1-loading}
d1 <- read.csv(file="../data/femmer-2014.csv")

# cast categorical variables to actual factors
cat.exp <- c("no experience", "up to 6 months", "6 to 12 months", "more than 12 months")
d1 <- d1 %>% 
  mutate(
    ExpProgAca <- factor(ExpProgAca, levels=cat.exp, ordered=TRUE),
    ExpProgInd <- factor(ExpProgInd, levels=cat.exp, ordered=TRUE), 
    ExpSEAca <- factor(ExpSEAca, levels=cat.exp, ordered=TRUE),
    ExpSEInd <- factor(ExpSEInd, levels=cat.exp, ordered=TRUE),
    ExpREAca <- factor(ExpREAca, levels=cat.exp, ordered=TRUE),
    ExpREInd <- factor(ExpREInd, levels=cat.exp, ordered=TRUE)
  )
```

The following figure shows a simple visualization of the distribution of the number of missing domain elements (actors, domain objects, and associations) for the `active` and `passive` treatment of the requirements sentences.

```{r data-d1-visualization}
d1 %>% 
  select(passive, actors.missing, objects.missing, associations.missing) %>% 
  pivot_longer(
    cols = c(actors.missing, objects.missing, associations.missing),
    names_to = "dependent.variable",
    values_to = "count"
  ) %>% 
  ggplot(aes(y=dependent.variable, x=count, color=passive)) +
    geom_boxplot()
```

### Method m1

Finally, the authors performed a null-hypothesis significance test (NHST) to see if there is a statistically significant difference in the number of missing elements between the active or passive group.
Of particular interest to this synthesis is the difference in the number of missing _associations_.
The authors use the Mann-Whitney U test (i.e., a Wilcoxon rank-sum test) with a 95% confidence interval since the data is not normally distributed.

```{r method-m1-original}
wilcox.test(
  x = filter(d1, passive==1)[["associations.missing"]],
  y = filter(d1, passive==0)[["associations.missing"]],
  conf.int = TRUE,
  conf.level = 0.95,
  paired = FALSE
)
```

Evidence $e_1=E(h_1, d_1, m_1)$ concludes that passive voice has a statistically significant impact on the number of missing associations $Asc^-$ with a p-value of around 0.001.

## Evidence e2: Reanalysis 

Since statistical null-hypothesis significance tests are equivalent to a linear model,[^4] we can replace the Wilcoxon rank sum test with a linear model, i.e., `stats::lm`.

### Method m2

We have two means (one per `passive` value) and a non-parametric outcome variable.
We do not need to transform the variable `associations.missing` to ranks since the values are already equivalent to ranks.
This way, we do not lose any information through the rank transformation and the resulting confidence interval remains interpretable.

```{r method-m2}
# explicit definition of the causal hypothesis
h1 <- associations.missing ~ passive

# defining a linear model with the same causal hypothesis h1 and the previous data d1
e2 <- lm(
  formula = h1, 
  data = d1)

# investigating the model parameters
summary(e2)
```

From the parameters of the fitted linear model, we can obtain the confidence interval of the factor `passive` on the response variable `Asc^-`.

```{r evidence-e2}
confint(
  e2, 
  parm="passiveTRUE", 
  level=0.95)
```

The 95%-confidence interval is strictly positive, i.e., passive voice has a significant impact on the number of missing associations.
As such, $e_1=E(h_1, d_1, m_2)$ agrees with $e_1$.
This reanalysis, also referred to as a _test of robustness_, suggests that the conclusions drawn from $e_1$ about the impact of passive voice on the number of missing associations from a domain model is valid.

## Evidence e3: Revision

A follow-up study[^2] voiced concerns about the causal assumptions in the original study.

### Hypothesis h2

The study revised several causal assumptions about the original study.
One of them is that missing actors and objects in a domain model might increase to missing associations, since at least one of the nodes in the graph is missing.

```{r hypothesis-h2-dag}
h2.dag <- dagify(
  mact ~ pv,
  mobj ~ pv,
  masc ~ pv + mact + mobj,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", masc="missing.associations"),
  coords = list(x=c(pv=0.7, mact=2, mobj=2, masc=2),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1))
)

ggdag_status(h2.dag, use_labels = "label", text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

### Conclusion

Deriving a linear model from this DAG results in the following formula and confidence interval.

```{r evidence-e3}
# explicit definition of the new causal hypothesis h2
h2 <- associations.missing ~ passive + actors.missing + objects.missing

# defining a linear model with the new causal hypothesis h2 and the previous data d1
e3 <- lm(formula = h2, data = d1)
confint(e3, parm="passiveTRUE", level=0.95)
```

Despite revised causal assumption, the conclusion is still similar to $e2$, though a bit more cautious.
To determine which model explains the observed data better, though we can investigate the coefficient of determination $R^2$.
This coefficient represents the proportion of variation in the dependent variable that can be attributed to the independent variable

```{r comparison-e2-e3}
summary(e3)
```

The coefficient of determination (Multiple R-squared) of $e_3$ is greater than of $e_2$.

$$R^2_{e_2} = 0.1041 < 0.1606 = R^2_{e_3}$$

Because $h_2$ explains the observed data $d_1$ better than $h_1$, we assume it to be the hypothesis with the highest internal validity at this point.
Additionally, the coefficients indicate that the number of missing objects may have a statistically significant impact on the number of missing associations, confirming the additional causal assumption.

## Evidence e4: Revision and Reanalysis

Another revision in scope of the follow-up study was the consideration of random effects.

### Hypothesis h3

The assumptions are based on the fact that the relatively small sample of 15 participants included both better and worse performing students, and that the individual complexity of each requirement may not be comparable.

```{r hypothesis-h3-dag}
h3.dag <- dagify(
  mact ~ pv + skill + req,
  mobj ~ pv + skill + req,
  masc ~ pv + mact + mobj + skill + req,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", 
             masc="missing.associations", skill="Individual Skill", 
             req="Requirements Complexity"),
  coords = list(x=c(pv=0.7, mact=2, mobj=2, masc=2, skill=0, req=0),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1, skill=1.5, req=0.5))
)

ggdag_status(h3.dag, 
             use_labels = "label", 
             text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

These two factors are typically represented as random factors in a linear model.

```{r hypothesis-h3-formula}
# explicit definition of the new causal hypothesis
h3 <- associations.missing ~ passive + # main factor of interest
  actors.missing + objects.missing + # confounders
  (1|PID) + (1|RID) # random effects
```

### Conclusion

Because the inclusion of random factors in a linear model requires using a linear mixed model (LMM), the revision entails a reanalysis, i.e., replacing $h_2$ with $h_3$ also necessitates replacing $m_2$ (LM) with $m_3$ (LMM) via `lme3:lmer`.

```{r evidence-e4}
e4 <- lmer(formula = h3, data = d1)

confint(e4, level=0.95)
```

The confidence interval of the `passive` factor from $e_4$ is a lot more cautious than in $e_3$ but still strictly positive.
Evidence $e_4$ still supports $e_3$ in that passive voice has a statistically significant impact on the number of missing associations in a domain model.
Additionally, the factor `objects.missing` remains strictly positive, supporting the assumption that the number of missing objects impacts the number of missing associations.

```{r evidence-e4-mobj}
confint(e4, parm="objects.missing", level=0.95)
```

## Evidence e5: Revision

The revision of the hypothesis $h_1$ of the original study included one more assumption.
Academic and industrial experience in requirements engineering might reduce the number of missing associations, as they increase the likelihood of previous modeling experience.

### Hypothesis h4

These additional assumptions can be visualized in the following DAG.

```{r hypothesis-h4-dag}
h4.dag <- dagify(
  mact ~ pv + exp.aca + exp.ind + skill + req,
  mobj ~ pv + exp.aca + exp.ind + skill + req,
  masc ~ pv + mact + mobj + exp.aca + exp.ind + skill + req,
  
  exposure = "pv",
  outcome = "masc",
  labels = c(pv="passive.voice", mact="missing.actors", mobj="missing.objects", 
             masc="missing.associations", exp.aca="Academic Experience in RE", 
             exp.ind="Industrial Experience in RE", skill="Individual Skill", 
             req="Requirements Complexity"),
  coords = list(x=c(pv=0.7, mact=2, mobj=2, masc=2, exp.aca=0.3, exp.ind=0.3, skill=0, req=0),
                y=c(pv=1, mact=1.5, mobj=0.5, masc=1, exp.aca=-0.5, exp.ind=0, skill=1.5, req=0.5))
)

ggdag_status(h4.dag, use_labels = "label",text = FALSE) +
  guides(fill = "none", color = "none") + 
  theme_dag()
```

These two factors are added as predictors to the formula for the linear model.

```{r hypothesis-h4-formula}
h4 <- associations.missing ~ passive + 
  actors.missing + objects.missing + 
  ExpREAca + ExpREInd + 
  (1|PID) + (1|RID)
```

### Conclusion

#### Model comparison

Fitting a linear mixed model to this new hypothesis produces evidence $e_5$.
To assess, whether the new hypothesis $h_4$ has a greater internal validity than $h_3$, we compare the $R^2$ values of the two fitted models. 

```{r evidence-e5}
e5 <- lmer(formula = h4, data = d1)

performance::r2(e4)
performance::r2(e5)
```

Since $R^2_{e_5} > R^2_{e_4}$, we conclude that $h_4$ is a better causal explanation of data $d_1$ than $h_3$.

#### Factor evaluation

To evaluate the conclusion generated by evidence $e_5$, we again generate confidence intervals.

```{r evidence-e5-ci}
ggcoef_model(e5)
```

When evaluating the conclusion of $e_5$ regarding the impact of the use of passive voice, we calculate the 95% confidence interval of the `passiveTRUE` factor.

```{r evidence-e5-passive}
confint(e5, parm="passiveTRUE", level=0.95)
```

This confidence interval is significantly different to the previous one. 
Though mostly positive, the confidence interval intersects 0 and is therefore not significant anymore.
Additionally, the only remaining factor with a strictly positive confidence interval is the number of missing objects `objects.missing`.

```{r evidence-e5-mobj}
confint(e5, parm="objects.missing", level=0.95)
```

The new evidence $e_5=E(h_4, d_1, m_3)$ is more internally valid than previous evidences and rejects the hypothesis that passive voice has a significant impact on the number of missing associations in domain modeling.

## Evidence e6: Reanalysis

Finally, the follow-up study also suggested to use a Bayesian modeling approach instead of a frequentist.
Bayesian approaches are understood to be more robust, sophisticated, and preserve uncertainty.[^5]

### Method m4

The Bayesian model, other than the linear mixed model, can represent the hypothesis properly as a Binomial probability distribution.

```{r hypothesis-h4-bayesian}
h4.b <- (associations.missing | trials(associations.expected) ~ 
           passive + actors.missing + objects.missing +
           ExpREAca + ExpREInd + (1|RID) + (1|PID))
```

For each of the selected predictors, we select an uninformative prior distribution that encodes our previous knowledge about the causal phenomenon.

```{r method-m4-priors}
priors <- c(prior(normal(-1, 1), class = Intercept),
            prior(normal(0, 1), class = b),
            prior(weibull(2, 1), class = sd, coef = Intercept, group = RID),
            prior(weibull(2, 1), class = sd, coef = Intercept, group = PID),
            prior(exponential(1), class = sd))
```

To assert that the priors are feasible, we sample from the priors without training the Bayesian model with the data.

```{r method-m4-model-prior, message=FALSE, warning=FALSE}
e6.prior <-
  brm(data = d1, family = binomial, h4.b, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/e6.prior"
  )
```

We visualize the results to graphically check the priors eligibility.

```{r method-m4-priorpc}
ndraws <- 400
brms::pp_check(e6.prior, type="bars", ndraws=ndraws)
```

The distribution of the predicted values for the response variable ($y_{rep}$) overlaps with the actually observed response variable distribution ($y$), confirming the feasibility of the priors.
Given the feasibility of the priors, we train the regression model. 
This process updates the prior distributions of all predictor coefficients.

```{r method-m4-model, message=FALSE, warning=FALSE}
e6 <-
  brm(data = d1, family = binomial, h4.b, prior = priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4,
    file = "fits/e6"
  )
```

We sample from the posterior distributions similar to the prior predictive check.

```{r method-m4-postpc}
brms::pp_check(e6, type="bars", ndraws=ndraws)
```

### Conclusion

We evaluate the trained model. 
An initial glance at the predictor coefficients shows their posterior distribution.

```{r evidence-e6-summary}
# numeric summary of the confidence intervals in the trained model
# summary(e6)

# visual summary of the confidence intervals in the trained model
mcmc_plot(e6)
```

The confidence interval of the factor passive voice `b_passiveTRUE` again overlaps 0 and is, therefore, not significant.
Again, the confidence interval of the number of missing objects `b_objects.missing` is strictly positive, indicating a significant impact.
Similarly, the intercepts of the random effects per participant `sd_PID__Intercept` and per requirements `sd_RID__Intercept` are strictly positive, also hinting at a significant factor.

Confidence intervals do not show the full picture, though.
To involve all uncertainty that the model has picked up during the training in the conclusion, we can visualize the marginal effect of passive voice via `brms::conditional_effects`.

```{r evidence-e6-marginaleffect}
conditional_effects(e6, effects="passive", trials=1)
```

The overlap between the two confidence intervals shows that the use of passive voice does not significantly cause more missing associations.

## Evidence e7: Replication

A second follow-up study collected new data about the hypothesis.

### Data d2

During this study[^3], the researchers conducted a crossover-design experiment involving 25 participants, mostly from industry.
The experimental task was the same (i.e., deriving domain models from natural language requirements sentences), but the material differed.
Additionally, due to the crossover design, participants were not divided into a treatment and control group, but received all levels of the treatment (just in different orders).
Also, the experiment involved another treatment (the use of _ambiguous pronouns_) which is not relevant for the current phenomenon.
Hence, we discard it.

```{r data-d2-loading}
d2 <- read.csv(file="../data/frattini-2024.csv")

# determine the values of the categorical variables
categories.degree <- c("High-School", "Bachelor's degree", "Master's degree")#, "Ph.D.")
categories.domain.knowledge.os <- 1:5
categories.domain.knowledge.db <- 2:5
categories.occurrence <- c("None", "Rarely", "From time to time", "Often")
categories.roles <- c("role.RE", "role.Arch", "role.Dev", "role.MGT", "role.none")
  
# cast the categorical variables to factors such that they are recognized properly
d2 <- d2 %>% 
  mutate(
    RQ = factor(RQ, levels=0:3, ordered=FALSE),
    edu = factor(edu, levels=categories.degree, ordered=TRUE),
    dom.os = factor(dom.os, levels=categories.domain.knowledge.os, ordered=TRUE),
    dom.db = factor(dom.db, levels=categories.domain.knowledge.db, ordered=TRUE),
    model.occ = factor(model.occ, levels=categories.occurrence, ordered=TRUE),
    tool = factor(tool, levels=categories.occurrence, ordered=TRUE),
    
    primary.role = factor(primary.role, levels=categories.roles, ordered=FALSE)
  ) %>% 
  rename(all_of(c(passive = "PassiveVoice"))) %>% 
  filter(RQ %in% c(0, 1))
```

The following figure shows a simple visualization of the distribution of the number of missing domain elements for the `active` and `passive` treatment of the requirements sentences.
Actors and objects were combined to `entities` because there were not many actors in the requirements artifacts.

```{r data-d2-visualization}
d2 %>% 
  select(passive, entities.missing, associations.missing) %>% 
  pivot_longer(
    cols = c(entities.missing, associations.missing),
    names_to = "dependent.variable",
    values_to = "count"
  ) %>% 
  ggplot(aes(y=dependent.variable, x=count, color=passive)) +
    geom_boxplot()
```

### Conclusion

The new data allows to produce evidence $e_7$ that acts as a replication of $e_1$, i.e., uses the same hypothesis $h_1$ and the same analysis method $m_1$, but data $d_2$ instead of $d_1$.
Due to the crossover design, the data is paired, however.

```{r data-d2-paired}
d2.paired <- d2 %>% 
  select(PID, RQ, associations.missing) %>% 
  reshape(idvar=c("PID"), timevar="RQ", direction="wide") %>% 
  rename(all_of(c(baseline = "associations.missing.0",
                  treatment = "associations.missing.1")))
```

To produce evidence $e_7=E(h_1, d_2, m_1)$, we need to adapt $m_1$ for paried data (i.e., set `paired = TRUE`).

```{r evidence-e7}
wilcox.test(
  x=d2.paired$baseline, 
  y=d2.paired$treatment,
  conf.int = TRUE,
  conf.level = 0.95,
  paired = TRUE
)
```

The resulting p-value of 0.026 differs slightly from the p-value of $e_1$ (0.001), but with a significance level of $\alpha=0.05$, $e_1$ and $e_7$ would agree.

## Evidence e8: Replication

The new data also allows to produce evidence $e_8$ that acts as a replication of $e_4$, i.e., uses the same hypothesis $h_3$ and the same analysis method $m_3$, but data $d_2$ instead of $d_1$.
Since actors and objects were combined to entities in $d_2$, we need to slightly adapt the hypothesis.

```{r hypothesis-h3-combined}
h3.c <- associations.missing ~ passive + # main factor of interest
  entities.missing + # confounder
  (1|PID) + (1|RID) # random effects
```

To make the evaluation easier to compare, we can re-evaluate $e_4$ with this hypothesis.

```{r data-d1-combined}
d1.c <- d1 %>% mutate(
  entities.missing = actors.missing + objects.missing,
  entities.expected = actors.expected + objects.expected
  )
```

```{r evidence-e4-combined}
e4.c <- lmer(formula = h3.c, data = d1.c)

ggcoef_compare(list("Original evidence e4" = e4, 
                    "Combined evidence e4.c" = e4.c), 
               include = c("passive", "actors.missing", "objects.missing", "entities.missing"))
```

As the forest plot fo coefficients shows, the combination does not affect the assumed impact of passive voice on the outcome variable.
The combination of actors and objects missing results in a slightly more cautious impact of missing entities on missing associations.

### Conclusion

Due to the design of the second experiment, the hypothesis needs to be adapted slightly.
The random effect `(1|RID)` must be removed, as the treatment `passive` is perfectly correlated with the ID of the requirements `RID`, i.e., every requirement has a particular level of the treatment.

```{r hypothesis-h3-no-difficulty}
h3.c.2 <- associations.missing ~ passive + # main factor of interest
  entities.missing + # confounder
  (1|PID) # random effects
```

Now, we can fit another LMM $e_8$ and compare it to the slightly adapted $e_{4.c}$.

```{r evidence-e8}
e8 <- lmer(formula = h3.c.2, data = d2)

ggcoef_compare(list("Evidence e4.c" = e4.c, 
                    "Replication e8" = e8), 
               include = c("passive", "entities.missing"))
```

The conclusions differ vastly.
While $e_{4.c}$ still assumes a significant impact of the use of passive voice on the number of missing associations, $e_8$ strongly disagrees.
On the other hand, the assumed impact of the number of missing entities on the number of missing associations is assumed to be much stronger.

## Evidence e9: Revision

The previous disagreement of conclusions requires a revision of the hypotheses.
At least one relevant variable is missing from $h_4$ (or its variants) which would explain the differences.
Consequently, the second follow-up study proposes a revised hypothesis with several additional factors that potentially explain the difference between the conclusions from $e_{4.c}$ and $e_8$.

```{r hypothesis-h5}
h5 <- associations.missing ~ 1 + passive +
  entities.missing + (1|PID) + 
  RQ*period +
  duration.scaled + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ +
  dom.os + dom.db
```

The fit model produces the following coefficients.

```{r evidence-e9}
e9 <- lmer(formula = h5, data = d2)
ggcoef_model(e9)
```

The coefficients indicate that passive voice is again not significant, while the model supports the assumption that the missing entities do influence the number of missing associations.
To compare the two hypotheses, we again calculate their $R^2$-scores.

```{r evidence-e9-comparison}
performance::r2(e8)
performance::r2(e9)
```

While the marginal $R^2$ value of $e_9$ is greater than that of $e_8$ (which suggests that $h_5$ has higher internal validity than $h_4$), the diagnostic output indicates that $e_9$ might be a singular fit.

```{r evidence-e9-singular}
lme4::isSingular(e9)
```

The check via `lme4::isSingular` confirms this. 
The singularity of $e9$ indicates that a LMM is not the right method for analyzing the data $d_2$ under the hypothesis $h_5$. 
This calls for a reanalysis.

## Evidence e10: Reanalysis

Similar to how $e_6$ improved over $e_5$ by replacing the analysis method $m_3$ of linear mixed models with $m_4$, Bayesian models, we produce $e_{10}=E(h_5, d_2, m_4)$.
The Bayesian model is not only a better method from a validity standpoint, it can also deal with the encountered singularity.
Again, it allows to adjust the hypothesis $h_5$ to reflect the binomial nature of the data generation process.

```{r hypothesis-h5-bayesian}
h5.b <- associations.missing | trials(associations.expected) ~ 1 + 
  (1|PID) + 
  RQ*period +
  entities.missing +
  duration.scaled + 
  exp.se.scaled + exp.re.scaled + edu + primary.role + 
  model.train + model.occ +
  dom.os + dom.db
```

Similarly as with evidence $e_6$, we define priors and perform a prior predictive check to assess its validity

```{r evidence-e10-prior}
e10.priors <- c(
  prior(normal(-1.7, 0.4), class=Intercept),
  prior(normal(0, 0.5), class=b, coef="RQ1"),
  prior(normal(0, 0.4), class=b),
  prior(weibull(2, 0.5), class=sd)
)

e10.prior <-
  brm(data = d2, family = binomial, h5.b, prior = e10.priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only",
    file = "fits/e10.prior"
  )

brms::pp_check(e10.prior, type="bars", ndraws=ndraws)
```

Given the appropriateness of the priors, we can fit the model and perform a posterior predictive check.

```{r evidence-e10}
e10 <-
  brm(data = d2, family = binomial, h5.b, prior = e10.priors,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, 
    file = "fits/e10"
  )

brms::pp_check(e10, type="bars", ndraws=ndraws)
```

### Conclusion

The conclusions of evidence $e_10$ can again be visualized via an MCMC plot.

```{r evidence-e10-conclusion}
mcmc_plot(e10)
```

The confidence intervals of the coefficients again support the following conclusions:

1. The use of passive voice has a slight positive, yet not statistically significant impact on the number of missing associations.
2. The number of missing entities has a statistically significant impact on the number of missing associations.

This is in line with evidence $e_6$ and allows us to conclude that the current state of evidence supports the aforementioned findings.

[^1]: Femmer, H., Kučera, J., & Vetrò, A. (2014, September). On the impact of passive voice requirements on domain modelling. In Proceedings of the 8th ACM/IEEE international symposium on empirical software engineering and measurement (pp. 1-4). https://doi.org/10.1145/2652524.2652554
[^2]: Frattini, J., Fucci, D., Torkar, R., & Mendez, D. (2024, April). A second look at the impact of passive voice requirements on domain modeling: Bayesian reanalysis of an experiment. In Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering (pp. 27-33). https://doi.org/10.1145/3643664.3648211
[^3]: Frattini, J., Fucci, D., Torkar, R., Montgomery, L., Unterkalmsteiner, M., Fischbach, J., & Mendez, D. (2024). Applying Bayesian Data Analysis for Causal Inference about Requirements Quality: A Replicated Experiment. arXiv preprint: https://arxiv.org/abs/2401.01154
[^4]: https://lindeloev.github.io/tests-as-linear/
[^5]: Furia, C. A., Feldt, R., & Torkar, R. (2019). Bayesian data analysis in empirical software engineering research. IEEE Transactions on Software Engineering, 47(9), 1786-1810.